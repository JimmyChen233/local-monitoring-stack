apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prometheus-operator
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default-project
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: 61.8.0
    chart: kube-prometheus-stack
    values: |-
    # document for values.yaml: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
    # for the "grafana:" section, all values from the grafana sub-chart can be used, found here: https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
    # example grafana.ini configuration: https://github.com/prometheus-community/helm-charts/issues/2837
    fullnameOverride: "system-prometheus"
    cleanPrometheusOperatorObjectNames: true
    crds:
      enabled: false
    
    prometheusOperator:
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
      logLevel: info
      priorityClassName: dsh-daemon
      resources:
        requests:
          cpu: 50m
          memory: 100Mi
        limits:
          memory: 100Mi
    
      prometheusConfigReloader:
        resources:
          requests:
            cpu: 200m
            memory: 50Mi
          limits:
            cpu: 0  # this is a special override in the prometheus operator https://github.com/prometheus-operator/prometheus-operator/blob/0a93915921d2c4b1ac05db1bd4a1096b2026db08/cmd/operator/main.go#L174
            memory: 50Mi
    
    defaultRules:
      labels:
        prometheus-selector-target: system-prometheus
      enabled: true
      rules:
        kubernetesApps: true  # we put a custom version in our on monitoring repo
        kubeApiserverHistogram: true  # creates a lot of load
        kubeApiserverSlos: true  # creates a lot of load
      disabled:
        InfoInhibitor: true
        KubePersistentVolumeFillingUp: true
    
    alertmanager:
      enabled: true
    
    grafana:
      enabled: false
      forceDeployDashboards: true
    
    kubelet:
      enabled: true
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
        cAdvisorMetricRelabelings:
          # Drop less useful container CPU metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
          # Drop less useful container / always zero filesystem metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
          # Drop less useful / always zero container memory metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_memory_(mapped_file|swap)'
          # Drop less useful container process metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_(file_descriptors|tasks_state|threads_max)'
          # Drop container spec metrics that overlap with kube-state-metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_spec.*'
          # Drop cgroup metrics with no pod.
          - sourceLabels: [id, pod]
            action: drop
            regex: '.+;'
        cAdvisorRelabelings:
          - action: replace
            sourceLabels: [__address__]
            targetLabel: hostname
            separator: ":"
            regex: "(.*):(.*)"
            replacement: "$1"
          # needed for some "k8s.rules" recording rules
          - targetLabel: metrics_path
            sourceLabels:
              - "__metrics_path__"
    kubeApiServer:
      enabled: true
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
    
        metricRelabelings:
          # Drop excessively noisy apiserver buckets.
          - action: drop
            regex: apiserver_request_slo_duration_seconds_bucket
            sourceLabels:
            - __name__
          - action: drop
            regex: apiserver_request_sli_duration_seconds_bucket
            sourceLabels:
            - __name__
          - action: drop
            regex: etcd_request_duration_seconds_bucket
            sourceLabels:
            - __name__
    
    kubeControllerManager:
      enabled: false
      # we don't have it running
    coreDns:
      enabled: false
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
    kubeEtcd:
      enabled: true
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
    kubeScheduler:
      enabled: false
      # we don't have it running
    kubeProxy:
      enabled: false
    kubeStateMetrics:
      enabled: true
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
    nodeExporter:
      enabled: true
    prometheus-node-exporter:
      fullnameOverride: "prometheus-node-exporter"
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          memory: 64Mi
      extraArgs:
        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
        - --collector.ethtool
        - --collector.processes
      prometheus:
        monitor:
          enabled: false
    
    prometheus:
      enabled: true
      thanosService:
        enabled: true
      thanosServiceMonitor:
        enabled: true
        additionalLabels:
          prometheus-selector-target: system-prometheus
      serviceAccount:
        create: true
        name: "prometheus"
    
      serviceMonitor:
        additionalLabels:
          prometheus-selector-target: system-prometheus
      podDisruptionBudget:
        enabled: true
      prometheusSpec:
        logLevel: info
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelector:
          matchLabels:
            prometheus-selector-target: system-prometheus
        podMonitorNamespaceSelector: {}
        podMonitorSelector:
          matchLabels:
            prometheus-selector-target: system-prometheus
        ruleNamespaceSelector: {}
        ruleSelector:
          matchLabels:
            prometheus-selector-target: system-prometheus
        probeNamespaceSelector: {}
        probeSelector:
          matchLabels:
            prometheus-selector-target: system-prometheus
        scrapeConfigNamespaceSelector: {}
        scrapeConfigSelector:
          matchLabels:
            prometheus-selector-target: system-prometheus
        image:
          registry: registry.cp.kpn-dsh.com/quay.io
        resources:
          requests:
            cpu: 200m
            memory: 1Gi
          limits:
            memory: 1Gi
        containers:
        - name: "prometheus"
          env:
          - name: GOMEMLIMIT
            # set GOMEMLIMIT: https://weaviate.io/blog/gomemlimit-a-game-changer-for-high-memory-applications
            # this is a complex function to automatically set this to 80% of what the pod limit is.
            value: "819MiB"
        replicas: 2
        enableRemoteWriteReceiver: false
        enableAdminAPI: false
        walCompression: false
        retention: 1d
    
        additionalLabels:
          prometheus-selector-target: system-prometheus
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - ServerSideApply=true
      - Validate=false
